{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## PDF chatbot using gemini-1.5-pro\n",
        "\n",
        "1. ENTIRE PDF IS LOADED AS A SINGLE DOCUMENT\n",
        "2. SPLIT THE DATA INTO SMALLER CHUNKS WITH chunk_overlap AND SAVE INTO docs\n",
        "3. CONVERT THE DOCS INTO VECTOR-EMBEDDING\n",
        "4. APPLY THE EMBEDDING ON 'docs' and store in the chroma db\n",
        "5. FETCH THE DOCS RELATED TO QUESTIONS\n",
        "6. CREATE PROMPT\n",
        "7. CREATE A CHAIN TO EXECUTE THE TASK\n",
        "8. INPUT A QUESTION\n",
        "\n"
      ],
      "metadata": {
        "id": "c-xNsdwggrYI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain_community langchain-google-genai python-dotenv streamlit langchain_experimental sentence-transformers langchain_chroma langchainhub pypdf rapidocr-onnxruntime"
      ],
      "metadata": {
        "id": "NUtxq_mqRcz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.entire PDF is loaded as a single Document\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"yolov9_paper.pdf\")\n",
        "data = loader.load()  # entire PDF is loaded as a single Document"
      ],
      "metadata": {
        "id": "BbzYUUnWRvjy"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PDF file have 18 pages and each page have seperate doc - data\n",
        "print(\"Total pages:\",len(data))\n",
        "data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBEAlTm4R3wh",
        "outputId": "d3adbac8-e675-467c-9195-d8ce1922c5e3"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total pages: 18\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': 'yolov9_paper.pdf', 'page': 0}, page_content='YOLOv9: Learning What You Want to Learn\\nUsing Programmable Gradient Information\\nChien-Yao Wang1,2, I-Hau Yeh2, and Hong-Yuan Mark Liao1,2,3\\n1Institute of Information Science, Academia Sinica, Taiwan\\n2National Taipei University of Technology, Taiwan\\n3Department of Information and Computer Engineering, Chung Yuan Christian University, Taiwan\\nkinyiu@iis.sinica.edu.tw, ihyeh@emc.com.tw, and liao@iis.sinica.edu.tw\\nAbstract\\nToday’s deep learning methods focus on how to design\\nthe most appropriate objective functions so that the pre-\\ndiction results of the model can be closest to the ground\\ntruth. Meanwhile, an appropriate architecture that can\\nfacilitate acquisition of enough information for prediction\\nhas to be designed. Existing methods ignore a fact that\\nwhen input data undergoes layer-by-layer feature extrac-\\ntion and spatial transformation, large amount of informa-\\ntion will be lost. This paper will delve into the important is-\\nsues of data loss when data is transmitted through deep net-\\nworks, namely information bottleneck and reversible func-\\ntions. We proposed the concept of programmable gradi-\\nent information (PGI) to cope with the various changes\\nrequired by deep networks to achieve multiple objectives.\\nPGI can provide complete input information for the tar-\\nget task to calculate objective function, so that reliable\\ngradient information can be obtained to update network\\nweights. In addition, a new lightweight network architec-\\nture – Generalized Efficient Layer Aggregation Network\\n(GELAN), based on gradient path planning is designed.\\nGELAN’s architecture confirms that PGI has gained su-\\nperior results on lightweight models. We verified the pro-\\nposed GELAN and PGI on MS COCO dataset based ob-\\nject detection. The results show that GELAN only uses\\nconventional convolution operators to achieve better pa-\\nrameter utilization than the state-of-the-art methods devel-\\noped based on depth-wise convolution. PGI can be used\\nfor variety of models from lightweight to large. It can be\\nused to obtain complete information, so that train-from-\\nscratch models can achieve better results than state-of-the-\\nart models pre-trained using large datasets, the compari-\\nson results are shown in Figure 1. The source codes are at:\\nhttps://github.com/WongKinYiu/yolov9 .\\n1. Introduction\\nDeep learning-based models have demonstrated far bet-\\nter performance than past artificial intelligence systems in\\nvarious fields, such as computer vision, language process-\\ning, and speech recognition. In recent years, researchers\\nFigure 1. Comparisons of the real-time object detecors on MS\\nCOCO dataset. The GELAN and PGI-based object detection\\nmethod surpassed all previous train-from-scratch methods in terms\\nof object detection performance. In terms of accuracy, the new\\nmethod outperforms RT DETR [43] pre-trained with a large\\ndataset, and it also outperforms depth-wise convolution-based de-\\nsign YOLO MS [7] in terms of parameters utilization.\\nin the field of deep learning have mainly focused on how\\nto develop more powerful system architectures and learn-\\ning methods, such as CNNs [21–23, 42, 55, 71, 72], Trans-\\nformers [8, 9, 40, 41, 60, 69, 70], Perceivers [26, 26, 32, 52,\\n56, 81, 81], and Mambas [17, 38, 80]. In addition, some\\nresearchers have tried to develop more general objective\\nfunctions, such as loss function [5, 45, 46, 50, 77, 78], la-\\nbel assignment [10, 12, 33, 67, 79] and auxiliary supervi-\\nsion [18, 20, 24, 28, 29, 51, 54, 68, 76]. The above studies\\nall try to precisely find the mapping between input and tar-\\nget tasks. However, most past approaches have ignored that\\ninput data may have a non-negligible amount of informa-\\ntion loss during the feedforward process. This loss of in-\\nformation can lead to biased gradient flows, which are sub-\\nsequently used to update the model. The above problems\\ncan result in deep networks to establish incorrect associa-\\ntions between targets and inputs, causing the trained model\\nto produce incorrect predictions.\\n1arXiv:2402.13616v2  [cs.CV]  29 Feb 2024')"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. SPLIT THE DATA INTO SMALLER CHUNKS WITH chunk_overlap AND SAVE INTO docs\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# split data\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200\n",
        "    )\n",
        "\n",
        "docs = text_splitter.split_documents(data)\n",
        "\n",
        "# split data into smaller doc because we only want pick data relevant question\n",
        "print(\"Total number of documents: \",len(docs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xs51uV-HR7cz",
        "outputId": "f52687b8-5dd8-4d01-c94d-8255f840e37a"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of documents:  96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_aE3zJ4kWD9e",
        "outputId": "ad04a230-e8f3-42b2-bc91-5b6d8666914e"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': 'yolov9_paper.pdf', 'page': 0}, page_content='YOLOv9: Learning What You Want to Learn\\nUsing Programmable Gradient Information\\nChien-Yao Wang1,2, I-Hau Yeh2, and Hong-Yuan Mark Liao1,2,3\\n1Institute of Information Science, Academia Sinica, Taiwan\\n2National Taipei University of Technology, Taiwan\\n3Department of Information and Computer Engineering, Chung Yuan Christian University, Taiwan\\nkinyiu@iis.sinica.edu.tw, ihyeh@emc.com.tw, and liao@iis.sinica.edu.tw\\nAbstract\\nToday’s deep learning methods focus on how to design\\nthe most appropriate objective functions so that the pre-\\ndiction results of the model can be closest to the ground\\ntruth. Meanwhile, an appropriate architecture that can\\nfacilitate acquisition of enough information for prediction\\nhas to be designed. Existing methods ignore a fact that\\nwhen input data undergoes layer-by-layer feature extrac-\\ntion and spatial transformation, large amount of informa-\\ntion will be lost. This paper will delve into the important is-')"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"API-KEY\""
      ],
      "metadata": {
        "id": "KUiZNGw1Se3m"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.CONVERT THE DOCS INTO VECTOR-EMBEDDING\n",
        "\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "# store the gemeni api key into .env\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "#Get an API key:\n",
        "# Head to https://ai.google.dev/gemini-api/docs/api-key to generate a Google AI API key. Paste in .env file\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "vector = embeddings.embed_query(\"hello, world!\")  # THIS IS JUST FOR DEMO\n",
        "vector[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mv2fD_8hSV8D",
        "outputId": "90fa269b-8347-451f-b8d6-25e7856c568f"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.05168594419956207,\n",
              " -0.030764883384108543,\n",
              " -0.03062233328819275,\n",
              " -0.02802734449505806,\n",
              " 0.01813092641532421]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. APPLY THE EMBEDDING ON 'docs' and store in the chroma db\n",
        "# provide 'docs' and embedding model\n",
        "\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=docs,\n",
        "    embedding=GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\"))"
      ],
      "metadata": {
        "id": "Ogsl_-Y1SlBe"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. FETCH THE DOCS RELATED TO QUESTIONS\n",
        "\n",
        "retriever = vectorstore.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\"k\": 10})\n",
        "\n",
        "# users question\n",
        "# from all 96 doc it will retrive the top  10 related doc to question\n",
        "retrieved_docs = retriever.invoke(\"What is Real-time Unauthorized Access Detection?\")\n",
        "\n",
        "len(retrieved_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csGYQpSqSyua",
        "outputId": "b67b87ba-c8cf-4de7-9110-a97b00d3850d"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(retrieved_docs[5].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12HsX02nS6OX",
        "outputId": "700c5903-d83b-47c9-c9d3-ebef83f46db6"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ter performance than past artificial intelligence systems in\n",
            "various fields, such as computer vision, language process-\n",
            "ing, and speech recognition. In recent years, researchers\n",
            "Figure 1. Comparisons of the real-time object detecors on MS\n",
            "COCO dataset. The GELAN and PGI-based object detection\n",
            "method surpassed all previous train-from-scratch methods in terms\n",
            "of object detection performance. In terms of accuracy, the new\n",
            "method outperforms RT DETR [43] pre-trained with a large\n",
            "dataset, and it also outperforms depth-wise convolution-based de-\n",
            "sign YOLO MS [7] in terms of parameters utilization.\n",
            "in the field of deep learning have mainly focused on how\n",
            "to develop more powerful system architectures and learn-\n",
            "ing methods, such as CNNs [21–23, 42, 55, 71, 72], Trans-\n",
            "formers [8, 9, 40, 41, 60, 69, 70], Perceivers [26, 26, 32, 52,\n",
            "56, 81, 81], and Mambas [17, 38, 80]. In addition, some\n",
            "researchers have tried to develop more general objective\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# call the model\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-pro\",\n",
        "    temperature=0.3, # 0 = more focus related to answer & 1 = more diverse output (random o/p)\n",
        "    max_tokens=500)  # Number words in output"
      ],
      "metadata": {
        "id": "Jog49fDaTE2c"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. CREATE PROMPT\n",
        "\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "system_prompt = (\n",
        "    \"You are an assistant for question-answering tasks. \"\n",
        "    \"Use the following pieces of retrieved context to answer \"\n",
        "    \"the question. If you don't know the answer, say that you \"\n",
        "    \"don't know. Use three sentences maximum and keep the \"\n",
        "    \"answer concise.\"\n",
        "    \"\\n\\n\"\n",
        "    \"{context}\"   # context = outputs\n",
        ")\n",
        "\n",
        "# create a prompt template\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_prompt),\n",
        "        (\"human\", \"{input}\"),  # users question\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "2y7w6nsrTPxD"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. CREATE A CHAIN TO EXECUTE THE TASK\n",
        "\n",
        "question_answer_chain = create_stuff_documents_chain(\n",
        "    llm,   # provide model\n",
        "    prompt) # provide promt\n",
        "\n",
        "rag_chain = create_retrieval_chain(\n",
        "    retriever,       # retriever provide top 10 doc related to the question_answer_chain\n",
        "    question_answer_chain)"
      ],
      "metadata": {
        "id": "ZnmhNc-qTQsD"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  8. INPUT A QUESTION\n",
        "\n",
        "response = rag_chain.invoke({\"input\": \"Real-time Unauthorized Access Detection\"})\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RphIfvJiTTJu",
        "outputId": "39125f3b-db0b-46bc-a585-155d8c04fb13"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input': 'Real-time Unauthorized Access Detection', 'context': [Document(metadata={'page': 0, 'source': 'Digital meter reading.pdf'}, page_content='This\\nreal-time\\nnotification\\nnot\\nonly\\nbolsters\\nsystem\\nsecurity\\nbut\\nalso\\nfacilitates\\nswift\\nresponse\\nmeasures\\nagainst\\npotential\\ntampering\\nor\\nunauthorized\\nactivities,\\nensuring\\ndata\\nintegrity\\nand\\nreliability\\nat\\nall\\ntimes.\\nMoreover ,\\nwe\\nprovide\\nan\\noptional\\nyet\\ninvaluable\\nfeature\\nincorporating\\nthe\\nESP32\\nCamera\\nmodule.\\nUpon\\ndetecting\\nthe\\nopening\\nof\\nthe\\nmeter\\nbox,\\nthis\\nmodule\\ncaptures\\neither\\na\\npicture\\nor\\na\\nvideo,\\ncapturing\\nvisual\\nevidence\\nof\\nthe\\nactivity .\\nSubsequently ,\\nthe\\ncaptured\\nmedia\\nis\\nautomatically\\nforwarded\\nto\\nthe\\ndesignated\\nrecipient,\\nsuch\\nas\\nthe\\nmeter\\nowner\\nor\\nelectric\\ncompany .\\nThis\\nfeature\\nadds\\nan\\nextra\\nlayer\\nof\\nsecurity\\nand\\naccountability\\nto\\nmetering\\nprocesses,\\nfurther\\nsolidifying\\nour\\ncommitment\\nto\\nimproved\\nreliability ,\\nheightened\\ncustomer\\nsatisfaction,\\nand\\nseamless\\nalignment\\nwith\\nthe\\nongoing\\ndigitalization\\nand\\nautomation\\ntrends\\nacross\\nindustries,\\nespecially\\nin\\nthe\\nface\\nof\\nglobal\\nchallenges\\nlike\\nthe\\nCOVID-19\\npandemic.\\n●\\nFeatures:\\n1.\\nReal-time\\nUnauthorized\\nAccess\\nDetection:\\nOur'), Document(metadata={'page': 0, 'source': 'Digital meter reading.pdf'}, page_content='This\\nreal-time\\nnotification\\nnot\\nonly\\nbolsters\\nsystem\\nsecurity\\nbut\\nalso\\nfacilitates\\nswift\\nresponse\\nmeasures\\nagainst\\npotential\\ntampering\\nor\\nunauthorized\\nactivities,\\nensuring\\ndata\\nintegrity\\nand\\nreliability\\nat\\nall\\ntimes.\\nMoreover ,\\nwe\\nprovide\\nan\\noptional\\nyet\\ninvaluable\\nfeature\\nincorporating\\nthe\\nESP32\\nCamera\\nmodule.\\nUpon\\ndetecting\\nthe\\nopening\\nof\\nthe\\nmeter\\nbox,\\nthis\\nmodule\\ncaptures\\neither\\na\\npicture\\nor\\na\\nvideo,\\ncapturing\\nvisual\\nevidence\\nof\\nthe\\nactivity .\\nSubsequently ,\\nthe\\ncaptured\\nmedia\\nis\\nautomatically\\nforwarded\\nto\\nthe\\ndesignated\\nrecipient,\\nsuch\\nas\\nthe\\nmeter\\nowner\\nor\\nelectric\\ncompany .\\nThis\\nfeature\\nadds\\nan\\nextra\\nlayer\\nof\\nsecurity\\nand\\naccountability\\nto\\nmetering\\nprocesses,\\nfurther\\nsolidifying\\nour\\ncommitment\\nto\\nimproved\\nreliability ,\\nheightened\\ncustomer\\nsatisfaction,\\nand\\nseamless\\nalignment\\nwith\\nthe\\nongoing\\ndigitalization\\nand\\nautomation\\ntrends\\nacross\\nindustries,\\nespecially\\nin\\nthe\\nface\\nof\\nglobal\\nchallenges\\nlike\\nthe\\nCOVID-19\\npandemic.\\n●\\nFeatures:\\n1.\\nReal-time\\nUnauthorized\\nAccess\\nDetection:\\nOur'), Document(metadata={'page': 0, 'source': 'Digital meter reading.pdf'}, page_content='the\\nongoing\\ndigitalization\\nand\\nautomation\\ntrends\\nacross\\nindustries,\\nespecially\\nin\\nthe\\nface\\nof\\nglobal\\nchallenges\\nlike\\nthe\\nCOVID-19\\npandemic.\\n●\\nFeatures:\\n1.\\nReal-time\\nUnauthorized\\nAccess\\nDetection:\\nOur\\nsystem\\ndetects\\nand\\nreports\\nunauthorized\\naccess\\nto\\nmeter\\nboxes\\nin\\nreal-time.\\nImmediate\\nnotifications\\nare\\nsent\\nto\\nthe\\nmeter\\ncompany\\nupon\\ndetecting\\nany\\nunauthorized\\nactivity ,\\nsuch\\nas\\nopening\\nthe\\nmeter\\nbox.\\n2.\\nOptional\\nESP32\\nCamera\\nIntegration:\\nAn\\noptional\\nfeature\\nincludes\\nthe\\nintegration\\nof\\nthe\\nESP32\\nCamera\\nmodule.\\nWhen\\nthe\\nmeter\\nbox\\nis\\nopened,\\nthe\\ncamera\\ncaptures\\neither\\na\\npicture\\nor\\nvideo,\\nproviding\\nvisual\\nevidence\\nof\\nthe\\nactivity .\\nThe\\ncaptured\\nmedia\\nis\\nautomatically\\nsent\\nto\\nthe\\ndesignated\\nrecipient,\\nenhancing\\nsecurity\\nand\\naccountability .'), Document(metadata={'page': 0, 'source': 'Digital meter reading.pdf'}, page_content='the\\nongoing\\ndigitalization\\nand\\nautomation\\ntrends\\nacross\\nindustries,\\nespecially\\nin\\nthe\\nface\\nof\\nglobal\\nchallenges\\nlike\\nthe\\nCOVID-19\\npandemic.\\n●\\nFeatures:\\n1.\\nReal-time\\nUnauthorized\\nAccess\\nDetection:\\nOur\\nsystem\\ndetects\\nand\\nreports\\nunauthorized\\naccess\\nto\\nmeter\\nboxes\\nin\\nreal-time.\\nImmediate\\nnotifications\\nare\\nsent\\nto\\nthe\\nmeter\\ncompany\\nupon\\ndetecting\\nany\\nunauthorized\\nactivity ,\\nsuch\\nas\\nopening\\nthe\\nmeter\\nbox.\\n2.\\nOptional\\nESP32\\nCamera\\nIntegration:\\nAn\\noptional\\nfeature\\nincludes\\nthe\\nintegration\\nof\\nthe\\nESP32\\nCamera\\nmodule.\\nWhen\\nthe\\nmeter\\nbox\\nis\\nopened,\\nthe\\ncamera\\ncaptures\\neither\\na\\npicture\\nor\\nvideo,\\nproviding\\nvisual\\nevidence\\nof\\nthe\\nactivity .\\nThe\\ncaptured\\nmedia\\nis\\nautomatically\\nsent\\nto\\nthe\\ndesignated\\nrecipient,\\nenhancing\\nsecurity\\nand\\naccountability .'), Document(metadata={'page': 10, 'source': 'yolov9_paper.pdf'}, page_content='Grosse. The reversible residual network: Backpropagation\\nwithout storing activations. Advances in Neural Information\\nProcessing Systems (NeurIPS) , 2017. 2, 3\\n[17] Albert Gu and Tri Dao. Mamba: Linear-time sequence\\nmodeling with selective state spaces. arXiv preprint\\narXiv:2312.00752 , 2023. 1\\n[18] Chaoxu Guo, Bin Fan, Qian Zhang, Shiming Xiang, and\\nChunhong Pan. AugFPN: Improving multi-scale fea-\\nture learning for object detection. In Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR) , pages 12595–12604, 2020. 1, 3\\n[19] Qi Han, Yuxuan Cai, and Xiangyu Zhang. RevColV2: Ex-\\nploring disentangled representations in masked image mod-\\neling. Advances in Neural Information Processing Systems\\n(NeurIPS) , 2023. 2, 3\\n[20] Zeeshan Hayder, Xuming He, and Mathieu Salzmann.\\nBoundary-aware instance segmentation. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR) , pages 5696–5704, 2017. 1, 3'), Document(metadata={'page': 10, 'source': 'yolov9_paper.pdf'}, page_content='[5] Kean Chen, Weiyao Lin, Jianguo Li, John See, Ji Wang, and\\nJunni Zou. AP-loss for accurate one-stage object detection.\\nIEEE Transactions on Pattern Analysis and Machine Intelli-\\ngence (TPAMI) , 43(11):3782–3798, 2020. 1\\n[6] Yabo Chen, Yuchen Liu, Dongsheng Jiang, Xiaopeng Zhang,\\nWenrui Dai, Hongkai Xiong, and Qi Tian. SdAE: Self-\\ndistillated masked autoencoder. In Proceedings of the Euro-\\npean Conference on Computer Vision (ECCV) , pages 108–\\n124, 2022. 2\\n[7] Yuming Chen, Xinbin Yuan, Ruiqi Wu, Jiabao Wang, Qibin\\nHou, and Ming-Ming Cheng. YOLO-MS: rethinking multi-\\nscale representation learning for real-time object detection.\\narXiv preprint arXiv:2308.05480 , 2023. 1, 3, 7, 8\\n[8] Mingyu Ding, Bin Xiao, Noel Codella, Ping Luo, Jingdong\\nWang, and Lu Yuan. DaVIT: Dual attention vision trans-\\nformers. In Proceedings of the European Conference on\\nComputer Vision (ECCV) , pages 74–92, 2022. 1\\n[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,'), Document(metadata={'page': 10, 'source': 'yolov9_paper.pdf'}, page_content='Boundary-aware instance segmentation. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR) , pages 5696–5704, 2017. 1, 3\\n[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\\nDeep residual learning for image recognition. In Proceed-\\nings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition (CVPR) , pages 770–778, 2016. 1, 4, 8\\n[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\\nIdentity mappings in deep residual networks. In Proceedings\\nof the European Conference on Computer Vision (ECCV) ,\\npages 630–645. Springer, 2016. 1, 4\\n[23] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-\\nian Q Weinberger. Densely connected convolutional net-\\nworks. In Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition (CVPR) , pages\\n4700–4708, 2017. 1\\n[24] Kuan-Chih Huang, Tsung-Han Wu, Hung-Ting Su, and Win-\\nston H Hsu. MonoDTR: Monocular 3D object detection with'), Document(metadata={'page': 11, 'source': 'yolov9_paper.pdf'}, page_content='[44] Chengqi Lyu, Wenwei Zhang, Haian Huang, Yue Zhou,\\nYudong Wang, Yanyi Liu, Shilong Zhang, and Kai Chen.\\nRTMDet: An empirical study of designing real-time object\\ndetectors. arXiv preprint arXiv:2212.07784 , 2022. 8, 2, 3, 4\\n[45] Kemal Oksuz, Baris Can Cam, Emre Akbas, and Sinan\\nKalkan. A ranking-based, balanced loss function unify-\\ning classification and localisation in object detection. Ad-\\nvances in Neural Information Processing Systems (NeurIPS) ,\\n33:15534–15545, 2020. 1\\n[46] Kemal Oksuz, Baris Can Cam, Emre Akbas, and Sinan\\nKalkan. Rank & sort loss for object detection and instance\\nsegmentation. In Proceedings of the IEEE/CVF Interna-\\ntional Conference on Computer Vision (ICCV) , pages 3009–\\n3018, 2021. 1\\n[47] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali\\nFarhadi. You only look once: Unified, real-time object detec-\\ntion. In Proceedings of the IEEE/CVF Conference on Com-\\nputer Vision and Pattern Recognition (CVPR) , pages 779–\\n788, 2016. 3'), Document(metadata={'page': 2, 'source': 'yolov9_paper.pdf'}, page_content='2. Related work\\n2.1. Real-time Object Detectors\\nThe current mainstream real-time object detectors are the\\nYOLO series [2, 7, 13–15, 25, 30, 31, 47–49, 61–63, 74, 75],\\nand most of these models use CSPNet [64] or ELAN [65]\\nand their variants as the main computing units. In terms of\\nfeature integration, improved PAN [37] or FPN [35] is of-\\nten used as a tool, and then improved YOLOv3 head [49] or\\nFCOS head [57, 58] is used as prediction head. Recently\\nsome real-time object detectors, such as RT DETR [43],\\nwhich puts its fundation on DETR [4], have also been pro-\\nposed. However, since it is extremely difficult for DETR\\nseries object detector to be applied to new domains without\\na corresponding domain pre-trained model, the most widely\\nused real-time object detector at present is still YOLO se-\\nries. This paper chooses YOLOv7 [63], which has been\\nproven effective in a variety of computer vision tasks and\\nvarious scenarios, as a base to develop the proposed method.'), Document(metadata={'page': 12, 'source': 'yolov9_paper.pdf'}, page_content='neural networks. In Proceedings of the IEEE/CVF Confer-\\nence on Computer Vision and Pattern Recognition (CVPR) ,\\npages 1492–1500, 2017. 1\\n[73] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin\\nBao, Zhuliang Yao, Qi Dai, and Han Hu. SimMIM: A simple\\nframework for masked image modeling. In Proceedings of\\nthe IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR) , pages 9653–9663, 2022. 2\\n[74] Shangliang Xu, Xinxin Wang, Wenyu Lv, Qinyao Chang,\\nCheng Cui, Kaipeng Deng, Guanzhong Wang, Qingqing\\nDang, Shengyu Wei, Yuning Du, et al. PP-YOLOE: An\\nevolved version of YOLO. arXiv preprint arXiv:2203.16250 ,\\n2022. 3, 8, 2, 4\\n[75] Xianzhe Xu, Yiqi Jiang, Weihua Chen, Yilun Huang,\\nYuan Zhang, and Xiuyu Sun. DAMO-YOLO: A re-\\nport on real-time object detection design. arXiv preprint\\narXiv:2211.15444 , 2022. 3, 7, 2, 4\\n[76] Renrui Zhang, Han Qiu, Tai Wang, Ziyu Guo, Ziteng Cui, Yu\\nQiao, Hongsheng Li, and Peng Gao. MonoDETR: Depth-')], 'answer': 'The system detects unauthorized access to meter boxes in real-time. Upon detecting any unauthorized activity, such as opening the meter box, immediate notifications are sent to the meter company. This real-time notification system bolsters security and facilitates swift response measures against potential tampering. \\n'}\n"
          ]
        }
      ]
    }
  ]
}